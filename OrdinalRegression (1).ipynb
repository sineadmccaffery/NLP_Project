{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f2ZAiyLeTMKf",
        "outputId": "0e05593f-26b8-4fdc-b593-913d96b16486"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy import sparse\n",
        "from sklearn import linear_model\n",
        "from collections import Counter\n",
        "from textblob import TextBlob\n",
        "import numpy as np\n",
        "import operator\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "import math\n",
        "from scipy.stats import norm\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2yzVXw--TDlt",
        "outputId": "128f05ba-05d6-44ae-e979-41eaf67a05aa"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **MAJORITY CLASSIFIER**"
      ],
      "metadata": {
        "id": "anR3EJs2OdwQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Majority class classifier, which identifies the class used most frequently in the training data and predicts every data point in the evaluation data to be that class.\n"
      ],
      "metadata": {
        "id": "S8U77GEXOl4j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(filename):\n",
        "    X = []\n",
        "    Y = []\n",
        "    with open(filename, encoding=\"utf-8\") as file:\n",
        "        for line in file:\n",
        "            cols = line.split(\"\\t\")\n",
        "            idd = cols[0]\n",
        "            label = cols[2].lstrip().rstrip()\n",
        "            text = cols[1]\n",
        "\n",
        "            X.append(text)\n",
        "            Y.append(label)\n",
        "\n",
        "    return X, Y"
      ],
      "metadata": {
        "id": "VqXV9ITgOhff"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def majority_class(trainY, evalY):\n",
        "    labelCounts=Counter()\n",
        "    for label in trainY:\n",
        "        labelCounts[label]+=1\n",
        "    majority_class=labelCounts.most_common(1)[0][0]\n",
        "\n",
        "    correct=0.\n",
        "    for label in evalY:\n",
        "        if label == majority_class:\n",
        "            correct+=1\n",
        "    return majority_class, correct/len(evalY)"
      ],
      "metadata": {
        "id": "Ym7L-RDDOrG9"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainingFile = \"drive/MyDrive/train.txt\"\n",
        "devFile = \"drive/MyDrive/dev.txt\"\n",
        "testFile = \"drive/MyDrive/test.txt\"\n",
        "\n",
        "trainX, trainY=load_data(trainingFile)\n",
        "devX, devY=load_data(devFile)\n",
        "testX, testY=load_data(testFile)\n",
        "\n",
        "mc, mc_devAcc=majority_class(trainY, devY)\n",
        "_, mc_testAcc=majority_class(trainY, testY)\n",
        "\n",
        "print(\"Majority class: %s, dev accuracy: %.3f, test accuracy: %.3f\" % (mc, mc_devAcc, mc_testAcc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pccf858vOtjz",
        "outputId": "ee3b5b66-86b2-4743-aa67-e9bf342260b8"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Majority class: Strongly Worded, dev accuracy: 0.440, test accuracy: 0.380\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **ORDINAL REGRESSION**"
      ],
      "metadata": {
        "id": "95vSkmniOaBe"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yd1rM0rM7qL1"
      },
      "source": [
        "[Ordinal regression](https://en.wikipedia.org/wiki/Ordinal_regression) is a classification method for categories on an ordinal scale -- e.g. [1, 2, 3, 4, 5] or [G, PG, PG-13, R].  This notebook implements ordinal regression using the method of [Frank and Hal 2001](https://www.cs.waikato.ac.nz/~eibe/pubs/ordinal_tech_report.pdf), which transforms a k-multiclass classifier into k-1 binary classifiers (each of which predicts whether a data point is above a threshold in the ordinal scale -- e.g., whether a movie is \"higher\" than PG).  This method can be used with any binary classification method that outputs probabilities; here L2-regularizaed binary logistic regression is used.\n",
        "\n",
        "This notebook trains a model (on `train.txt`), optimizes L2 regularization strength on `dev.txt`, and evaluates performance on `test.txt`.  Reports test accuracy with 95% confidence intervals."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "ZyBwyGUP7qL7"
      },
      "outputs": [],
      "source": [
        "def load_ordinal_data(filename, ordering):\n",
        "    X = []\n",
        "    Y = []\n",
        "    orig_Y=[]\n",
        "    for ordinal in ordering:\n",
        "        Y.append([])\n",
        "\n",
        "    with open(filename, encoding=\"utf-8\") as file:\n",
        "        for line in file:\n",
        "            cols = line.split(\"\\t\")\n",
        "            idd = cols[0]\n",
        "            label = cols[2].lstrip().rstrip()\n",
        "            text = cols[1]\n",
        "\n",
        "            X.append(text)\n",
        "\n",
        "            index = ordering.index(label)\n",
        "            for i in range(len(ordering)):\n",
        "                if index > i:\n",
        "                    Y[i].append(1)\n",
        "                else:\n",
        "                    Y[i].append(0)\n",
        "            orig_Y.append(label)\n",
        "\n",
        "    return X, Y, orig_Y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "8ouOURYf7qL7"
      },
      "outputs": [],
      "source": [
        "class OrdinalClassifier:\n",
        "\n",
        "    def __init__(self, ordinal_values, feature_method, trainX, trainY, devX, devY, testX, testY, orig_trainY, orig_devY, orig_testY):\n",
        "        self.ordinal_values=ordinal_values\n",
        "        self.feature_vocab = {}\n",
        "        self.feature_method = feature_method\n",
        "        self.min_feature_count=2\n",
        "        self.log_regs = [None]* (len(self.ordinal_values)-1)\n",
        "\n",
        "        self.testX_raw = testX\n",
        "        self.devX_raw = devX\n",
        "\n",
        "        self.trainY=trainY\n",
        "        self.devY=devY\n",
        "        self.testY=testY\n",
        "\n",
        "        self.orig_trainY=orig_trainY\n",
        "        self.orig_devY=orig_devY\n",
        "        self.orig_testY=orig_testY\n",
        "\n",
        "        self.trainX = self.process(trainX, training=True)\n",
        "        self.devX = self.process(devX, training=False)\n",
        "        self.testX = self.process(testX, training=False)\n",
        "\n",
        "    # Featurize entire dataset\n",
        "    def featurize(self, data):\n",
        "        featurized_data = []\n",
        "        for text in data:\n",
        "            feats = self.feature_method(text)\n",
        "            featurized_data.append(feats)\n",
        "        return featurized_data\n",
        "\n",
        "    # Read dataset and returned featurized representation as sparse matrix + label array\n",
        "    def process(self, X_data, training = False):\n",
        "\n",
        "        data = self.featurize(X_data)\n",
        "\n",
        "        if training:\n",
        "            fid = 0\n",
        "            feature_doc_count = Counter()\n",
        "            for feats in data:\n",
        "                for feat in feats:\n",
        "                    feature_doc_count[feat]+= 1\n",
        "\n",
        "            for feat in feature_doc_count:\n",
        "                if feature_doc_count[feat] >= self.min_feature_count:\n",
        "                    self.feature_vocab[feat] = fid\n",
        "                    fid += 1\n",
        "\n",
        "        F = len(self.feature_vocab)\n",
        "        D = len(data)\n",
        "        X = sparse.dok_matrix((D, F))\n",
        "        for idx, feats in enumerate(data):\n",
        "            for feat in feats:\n",
        "                if feat in self.feature_vocab:\n",
        "                    X[idx, self.feature_vocab[feat]] = feats[feat]\n",
        "\n",
        "        return X\n",
        "\n",
        "\n",
        "    def train(self):\n",
        "        (D,F) = self.trainX.shape\n",
        "\n",
        "        for idx, ordinal_value in enumerate(self.ordinal_values[:-1]):\n",
        "            best_dev_accuracy=0\n",
        "            best_model=None\n",
        "            for C in [0.1, 1, 10, 100]:\n",
        "\n",
        "                log_reg = linear_model.LogisticRegression(C = C, max_iter=1000)\n",
        "                log_reg.fit(self.trainX, self.trainY[idx])\n",
        "                development_accuracy = log_reg.score(self.devX, self.devY[idx])\n",
        "                if development_accuracy > best_dev_accuracy:\n",
        "                    best_dev_accuracy=development_accuracy\n",
        "                    best_model=log_reg\n",
        "\n",
        "\n",
        "            self.log_regs[idx]=best_model\n",
        "\n",
        "    def test(self):\n",
        "        cor = tot = 0\n",
        "\n",
        "        counts=Counter()\n",
        "        preds=[None]*(len(self.ordinal_values)-1)\n",
        "        for idx, ordinal_value in enumerate(self.ordinal_values[:-1]):\n",
        "            preds[idx]=self.log_regs[idx].predict_proba(self.testX)[:,1]\n",
        "\n",
        "        preds=np.array(preds)\n",
        "        for data_point in range(len(preds[0])):\n",
        "            ordinal_preds=np.zeros(len(self.ordinal_values))\n",
        "            for ordinal in range(len(self.ordinal_values)-1):\n",
        "                if ordinal == 0:\n",
        "                    ordinal_preds[ordinal]=1-preds[ordinal][data_point]\n",
        "                else:\n",
        "                    ordinal_preds[ordinal]=preds[ordinal-1][data_point]-preds[ordinal][data_point]\n",
        "\n",
        "            ordinal_preds[len(self.ordinal_values)-1]=preds[len(preds)-1][data_point]\n",
        "\n",
        "            prediction=np.argmax(ordinal_preds)\n",
        "            counts[prediction]+=1\n",
        "            if prediction == self.ordinal_values.index(self.orig_testY[data_point]):\n",
        "                cor+=1\n",
        "            tot+=1\n",
        "\n",
        "        return cor/tot\n",
        "\n",
        "  # FOR DEBUGGING / EVALUATION...\n",
        "    def example_test(self, dataset=\"test\", use_override=True, use_rule_only=False):\n",
        "      if dataset == \"test\":\n",
        "        dataX = self.testX_raw\n",
        "        goldY = self.orig_testY\n",
        "      elif dataset == \"dev\":\n",
        "        dataX = self.devX_raw\n",
        "        goldY = self.orig_devY\n",
        "      else:\n",
        "        raise ValueError(\"dataset must be 'test' or 'dev'\")\n",
        "\n",
        "      cor = tot = 0\n",
        "      for i, sentence in enumerate(dataX):\n",
        "        if use_rule_only:\n",
        "          predicted_label = self.predict_by_score(sentence)\n",
        "        else:\n",
        "          predicted_label, _ = self.predict_one(sentence, override=use_override)\n",
        "\n",
        "        if predicted_label == goldY[i]:\n",
        "            cor += 1\n",
        "        tot += 1\n",
        "\n",
        "      return cor / tot\n",
        "\n",
        "    def predict_one(self, sentence, override=False):\n",
        "      feats = self.feature_method(sentence)\n",
        "\n",
        "      # Vectorize for model prediction\n",
        "      F = len(self.feature_vocab)\n",
        "      X = sparse.dok_matrix((1, F))\n",
        "      for feat in feats:\n",
        "          if feat in self.feature_vocab:\n",
        "              X[0, self.feature_vocab[feat]] = feats[feat]\n",
        "\n",
        "      # Get ordinal class probabilities\n",
        "      preds = [lr.predict_proba(X)[0][1] for lr in self.log_regs]\n",
        "      ordinal_probs = np.zeros(len(self.ordinal_values))\n",
        "      ordinal_probs[0] = 1 - preds[0]\n",
        "      for i in range(1, len(self.ordinal_values) - 1):\n",
        "          ordinal_probs[i] = preds[i - 1] - preds[i]\n",
        "      ordinal_probs[-1] = preds[-1]\n",
        "\n",
        "      model_margin = np.sort(ordinal_probs)[-1] - np.sort(ordinal_probs)[-2]\n",
        "      model_pred_idx = np.argmax(ordinal_probs)\n",
        "      model_label = self.ordinal_values[model_pred_idx]\n",
        "\n",
        "      if override:\n",
        "         # Rule-based score\n",
        "          scores = self.score_by_class(feats)\n",
        "          rule_label = max(scores, key=scores.get)\n",
        "          rule_conf = scores[rule_label]\n",
        "          # Thresholds per class\n",
        "          thresholds = {\n",
        "              \"Apathetic\": 1.5,\n",
        "              \"Moderate\": 2.0,\n",
        "              \"Strongly Worded\": 2.5,\n",
        "              \"Extreme\": 1.5\n",
        "          }\n",
        "\n",
        "          # Decide final label\n",
        "          override_applied = False\n",
        "          if (override and rule_label != model_label\n",
        "              and rule_conf > thresholds[rule_label]\n",
        "              and model_margin < 0.4):\n",
        "              return rule_label, ordinal_probs\n",
        "      return model_label, ordinal_probs\n",
        "\n",
        "    def print_feature_importance(self, top_n=15, high_only = False):\n",
        "      inv_vocab = {idx: feat for feat, idx in self.feature_vocab.items()}\n",
        "      for i, model in enumerate(self.log_regs):\n",
        "          print(f\"\\n--- Feature importances for {self.ordinal_values[i+1]} and above ---\")\n",
        "          coef = model.coef_[0]\n",
        "          sorted_idx = coef.argsort()\n",
        "\n",
        "          if not high_only:\n",
        "            print(\"\\nLowest-weighted features:\")\n",
        "            for idx in sorted_idx[:top_n]:\n",
        "                print(f\"{inv_vocab[idx]:<30} {coef[idx]:.3f}\")\n",
        "\n",
        "          print(\"\\nHighest-weighted features:\")\n",
        "          for idx in reversed(sorted_idx[-top_n:]):\n",
        "              print(f\"{inv_vocab[idx]:<30} {coef[idx]:.3f}\")\n",
        "\n",
        "    def score_by_class(self, feats):\n",
        "      scores = {\n",
        "          \"Apathetic\": 0.0,\n",
        "          \"Moderate\": 0.0,\n",
        "          \"Strongly Worded\": 0.0,\n",
        "          \"Extreme\": 0.0\n",
        "      }\n",
        "\n",
        "      subj = feats.get(\"sentiment_subjectivity\", 0)\n",
        "      hate = feats.get(\"has_hate_speech\", 0)\n",
        "      violence = feats.get(\"has_violent_terms\", 0)\n",
        "      ideological = feats.get(\"has_ideological_violence\", 0)\n",
        "      imperative = feats.get(\"imperative_present\", 0)\n",
        "      modal = feats.get(\"modal_present\", 0)\n",
        "      pron_we = feats.get(\"pronoun_we_group\", 0)\n",
        "      pron_they = feats.get(\"pronoun_they_group\", 0)\n",
        "      apathy = feats.get(\"apathetic_signal\", 0)\n",
        "      military = feats.get(\"has_military_terms\", 0)\n",
        "      terrorism = feats.get(\"terrorism_term\", 0)\n",
        "      group_blaming = feats.get(\"group_blaming_language\", 0)\n",
        "      extreme_pattern = feats.get(\"extreme_pattern_match\", 0)\n",
        "\n",
        "      # --- Rhetorical count from phrase indicators ---\n",
        "      rhetorical_keywords = [\n",
        "          \"we must\", \"fight\", \"rigged\", \"enemy\", \"take back\", \"stand up\",\n",
        "          \"we will never\", \"reclaim\", \"stop the steal\"\n",
        "      ]\n",
        "      rhet_count = sum([feats.get(f\"rhet_{phrase.replace(' ', '_')}\", 0) for phrase in rhetorical_keywords])\n",
        "\n",
        "      # Apathetic\n",
        "      scores[\"Apathetic\"] += 2.0 * apathy\n",
        "      scores[\"Apathetic\"] += max(0, 0.5 - subj)  # low subjectivity = apathetic\n",
        "      scores[\"Apathetic\"] -= 1.5 * imperative + modal + rhet_count\n",
        "      scores[\"Apathetic\"] -= 2.0 * violence + 2.0 * hate\n",
        "\n",
        "      # Moderate\n",
        "      scores[\"Moderate\"] += 1.0 * pron_we + 0.5 * modal\n",
        "      scores[\"Moderate\"] += max(0.5 - subj, 0.2)\n",
        "      scores[\"Moderate\"] += 1.0 if violence == 0 and hate == 0 else 0\n",
        "      scores[\"Moderate\"] -= 1.0 * violence + 1.0 * hate\n",
        "\n",
        "      # Strongly Worded\n",
        "      scores[\"Strongly Worded\"] += 1.5 * rhet_count + subj + imperative\n",
        "      scores[\"Strongly Worded\"] += 0.7 * pron_they + 0.7 * group_blaming\n",
        "      scores[\"Strongly Worded\"] += modal + 0.5 * hate\n",
        "\n",
        "      # Extreme\n",
        "      scores[\"Extreme\"] += 2.5 * violence + 2.5 * hate + 2.0 * ideological\n",
        "      scores[\"Extreme\"] += 1.0 * pron_they + 1.2 * military + 1.0 * terrorism\n",
        "      scores[\"Extreme\"] += 1.5 * imperative + 1.5 * extreme_pattern\n",
        "      scores[\"Extreme\"] += 0.5 * group_blaming  # group blame often escalates\n",
        "\n",
        "      return scores\n",
        "\n",
        "    def predict_by_score(self, sentence):\n",
        "      # Get features\n",
        "      feats = self.feature_method(sentence)\n",
        "\n",
        "      # Get heuristic scores\n",
        "      scores = self.score_by_class(feats)\n",
        "\n",
        "      # Pick label with highest score\n",
        "      predicted_label = max(scores, key=scores.get)\n",
        "\n",
        "      return predicted_label\n",
        "\n",
        "    def get_predictions(self):\n",
        "      dataX = self.testX_raw\n",
        "      goldY = self.orig_testY\n",
        "      X_sparse = self.testX\n",
        "\n",
        "      y_true = []\n",
        "      y_pred = []\n",
        "\n",
        "      preds = [lr.predict_proba(X_sparse)[:, 1] for lr in self.log_regs]\n",
        "      preds = np.array(preds)\n",
        "\n",
        "      for i in range(len(preds[0])):\n",
        "        ordinal_probs = np.zeros(len(self.ordinal_values))\n",
        "        ordinal_probs[0] = 1 - preds[0][i]\n",
        "        for j in range(1, len(self.ordinal_values) - 1):\n",
        "            ordinal_probs[j] = preds[j - 1][i] - preds[j][i]\n",
        "        ordinal_probs[-1] = preds[-1][i]\n",
        "\n",
        "        pred_idx = np.argmax(ordinal_probs)\n",
        "        gold_idx = self.ordinal_values.index(goldY[i])\n",
        "\n",
        "        y_pred.append(self.ordinal_values[pred_idx])\n",
        "        y_true.append(self.ordinal_values[gold_idx])\n",
        "\n",
        "      return y_true, y_pred"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# attempting to create a formula to predict with... not as good as improved_bow_featurized.\n",
        "def tone_score_featurized(text):\n",
        "    feats = Counter()\n",
        "    words = nltk.word_tokenize(text)\n",
        "    lowered = [w.lower() for w in words]\n",
        "    lowered_text = \" \".join(lowered)\n",
        "\n",
        "    # --- Sentiment (from TextBlob) ---\n",
        "    blob = TextBlob(text)\n",
        "    feats[\"sentiment_subjectivity\"] = blob.sentiment.subjectivity\n",
        "\n",
        "    # --- Hate speech / insult terms ---\n",
        "    hate_terms = {\"traitor\", \"invader\", \"scum\", \"vermin\", \"filth\", \"rapist\", \"illegals\",\n",
        "                  \"thugs\", \"cruel\", \"loser\", \"dumb\", \"hate\", \"evil\", \"criminals\", \"animals\",\n",
        "                  \"terrorists\", \"kill\", \"murder\", \"slaughter\", \"extremist\", \"extremists\"}\n",
        "    hate_count = sum(1 for word in lowered if word in hate_terms)\n",
        "    feats[\"has_hate_speech\"] = int(hate_count > 0)\n",
        "\n",
        "    # --- Violent Terms ---\n",
        "    violent_terms = {\"murder\", \"slaughter\", \"execute\", \"exterminate\", \"annihilate\",\n",
        "                     \"eradicate\", \"kill\", \"stab\", \"shoot\", \"lynch\", \"hunt\", \"abuse\", \"hang\"}\n",
        "    violent_count = sum(1 for word in lowered if word in violent_terms)\n",
        "    feats[\"has_violent_terms\"] = int(violent_count > 0)\n",
        "\n",
        "    # --- Ideological Violence ---\n",
        "    feats[\"has_ideological_violence\"] = int(\"beliefs\" in lowered and any(w in lowered for w in violent_terms))\n",
        "\n",
        "    # --- Custom rhetorical keyword features ---\n",
        "    rhetorical_phrases = {\n",
        "        \"we must\", \"fight\", \"rigged\", \"enemy\", \"take back\", \"stand up\",\n",
        "        \"we will never\", \"reclaim\", \"stop the steal\"\n",
        "    }\n",
        "    feats[\"rhetorical_count\"] = sum(1 for phrase in rhetorical_phrases if phrase in lowered_text)\n",
        "\n",
        "    # --- Pronoun skew ---\n",
        "    pronoun_counts = Counter(word for word in lowered if word in {\"we\", \"our\", \"us\", \"they\", \"them\", \"their\", \"theirs\"})\n",
        "    feats[\"pronoun_skew\"] = pronoun_counts[\"they\"] - pronoun_counts[\"we\"]\n",
        "\n",
        "    # --- Apathetic tone ---\n",
        "    apathetic_signals = [\"not a big deal\", \"doesn’t matter\", \"don't care\", \"move on\", \"isn't important\",\n",
        "                         \"do not care\", \"could be worse\", \"no point\", \"does not matter\", \"is not important\"]\n",
        "    feats[\"apathetic_signal\"] = sum(1 for phrase in apathetic_signals if phrase in lowered_text)\n",
        "\n",
        "    # --- Modals ---\n",
        "    modals = [\"must\", \"shall\", \"should\", \"need to\", \"ought to\", \"have to\"]\n",
        "    feats[\"modal_present\"] = sum(1 for modal in modals if modal in lowered_text)\n",
        "\n",
        "    # --- Imperatives ---\n",
        "    imperatives = [\"stand up\", \"take back\", \"fight\", \"join\", \"resist\", \"reject\"]\n",
        "    feats[\"imperative_present\"] = sum(1 for imp in imperatives if imp in lowered_text)\n",
        "\n",
        "    # --- Militaristic vocabulary ---\n",
        "    militaristic = {\"troops\", \"attack\", \"defend\", \"enemy\", \"war\", \"mission\", \"strike\", \"military\",\n",
        "                    \"terrorist\", \"missile\", \"bomb\", \"gun\", \"police\", \"officer\", \"army\", \"soldier\"}\n",
        "    feats[\"military_vocab_count\"] = sum(1 for word in lowered if word in militaristic)\n",
        "    feats[\"has_military_terms\"] = int(feats[\"military_vocab_count\"] > 0)\n",
        "\n",
        "    # --- Group Blame ---\n",
        "    blame_patterns = [\"they caused\", \"they ruined\", \"their fault\", \"they are the reason\", \"they must pay\", \"they made\", \"they did\"]\n",
        "    feats[\"group_blaming_language\"] = sum(1 for pattern in blame_patterns if pattern in lowered_text)\n",
        "\n",
        "    # --- Extreme Language ---\n",
        "    extreme_patterns = [\"must kill\", \"kill them\", \"murder them\", \"they are animals\",\n",
        "                        \"we will exterminate\", \"no right to live\", \"need to be wiped out\", \"they must suffer\"]\n",
        "    feats[\"extreme_pattern_match\"] = sum(1 for pattern in extreme_patterns if pattern in lowered_text)\n",
        "\n",
        "    # --- Terrorism terms ---\n",
        "    terrorism_terms = {\"isis\", \"qaeda\", \"terror\", \"terrorists\"}\n",
        "    feats[\"terrorism_term\"] = int(any(word in terrorism_terms for word in lowered))\n",
        "\n",
        "    # === Final tone score ===\n",
        "    tone_score = (\n",
        "        1.5 * feats[\"rhetorical_count\"] +\n",
        "        2.0 * feats[\"has_hate_speech\"] +\n",
        "        2.0 * feats[\"has_violent_terms\"] +\n",
        "        1.2 * feats[\"imperative_present\"] +\n",
        "        1.0 * feats[\"has_ideological_violence\"] +\n",
        "        0.8 * feats[\"pronoun_skew\"] +\n",
        "        0.7 * feats[\"modal_present\"] +\n",
        "        0.5 * feats[\"has_military_terms\"] +\n",
        "        1.0 * feats[\"sentiment_subjectivity\"] +\n",
        "        0.5 * feats[\"terrorism_term\"] +\n",
        "        1.2 * feats[\"group_blaming_language\"] +\n",
        "        1.5 * feats[\"extreme_pattern_match\"] -\n",
        "        1.5 * feats[\"apathetic_signal\"]\n",
        "    )\n",
        "\n",
        "    return {\"tone_score\": tone_score}\n"
      ],
      "metadata": {
        "id": "FyEtH0U3tGQH"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "UI0lNKyl7qL8"
      },
      "outputs": [],
      "source": [
        "def improved_bow_featurize(text):\n",
        "    feats = Counter()\n",
        "    words = nltk.word_tokenize(text)\n",
        "    lowered = [w.lower() for w in words]\n",
        "\n",
        "    # --- bag-of-words ---\n",
        "    stopwords_set = set(stopwords.words(\"english\"))\n",
        "\n",
        "    for word in lowered:\n",
        "        if word not in stopwords_set and len(word) > 2 and word.isalpha():\n",
        "            feats[f\"word_{word}\"] += 1\n",
        "\n",
        "    # --- Sentiment features (TextBlob) ---\n",
        "    blob = TextBlob(text)\n",
        "    feats[\"sentiment_polarity\"] = blob.sentiment.polarity\n",
        "    feats[\"sentiment_subjectivity\"] = blob.sentiment.subjectivity\n",
        "\n",
        "    # --- Hate speech / insult terms ---\n",
        "    hate_terms = {\"traitor\", \"invader\", \"scum\", \"vermin\", \"filth\", \"rapist\", \"illegals\", \"hell\", \"devils\"\n",
        "                  \"thugs\", \"cruel\", \"loser\", \"dumb\", \"hate\", \"evil\", \"criminals\", \"animals\",\n",
        "                  \"terrorists\", \"kill\", \"murder\", \"slaughter\", \"extremist\", \"extremists\"}\n",
        "    feats[\"hate_speech_vocab_count\"] = sum(1 for word in lowered if word in hate_terms)\n",
        "\n",
        "    # --- Violent Terms ---\n",
        "    violent_terms = {\"murder\", \"slaughter\", \"execute\", \"exterminate\", \"annihilate\", \"force\"\n",
        "                     \"eradicate\", \"kill\", \"stab\", \"shoot\", \"lynch\", \"hunt\", \"abuse\", \"hang\"}\n",
        "    feats[\"violent_action_count\"] = sum(1 for word in lowered if word in violent_terms)\n",
        "\n",
        "    # --- Ideological Violence ---\n",
        "    if \"beliefs\" in lowered and any(w in lowered for w in violent_terms):\n",
        "      feats[\"ideological_violence_flag\"] = 1\n",
        "\n",
        "    # --- Custom rhetorical keyword features ---\n",
        "    rhetorical_phrases = {\n",
        "        \"we must\": \"rhet_we_must\",\n",
        "        \"fight\": \"rhet_fight\",\n",
        "        \"rigged\": \"rhet_rigged\",\n",
        "        \"enemy\": \"rhet_enemy\",\n",
        "        \"take back\": \"rhet_take_back\",\n",
        "        \"stand up\": \"rhet_stand_up\",\n",
        "        \"we will never\": \"rhet_never_concede\",\n",
        "        \"reclaim\": \"rhet_reclaim\",\n",
        "        \"stop the steal\": \"rhet_stop_the_steal\"\n",
        "    }\n",
        "\n",
        "    lowered_text = \" \".join(lowered)\n",
        "    for phrase, feat_name in rhetorical_phrases.items():\n",
        "        if phrase in lowered_text:\n",
        "            feats[feat_name] += 1\n",
        "\n",
        "    # --- Pronoun distribution ---\n",
        "    first_person_plural = {\"we\", \"our\", \"us\"}\n",
        "    second_person = {\"you\", \"your\", \"yours\"}\n",
        "    third_person_plural = {\"they\", \"them\", \"their\", \"theirs\"}\n",
        "\n",
        "    for word in lowered:\n",
        "        if word in first_person_plural:\n",
        "            feats[\"pronoun_we_group\"] += 1\n",
        "        if word in second_person:\n",
        "            feats[\"pronoun_you\"] += 1\n",
        "        if word in third_person_plural:\n",
        "            feats[\"pronoun_they_group\"] += 1\n",
        "\n",
        "    # --- Apathetic tone ---\n",
        "    apathetic_signals = [\"not a big deal\", \"doesn’t matter\", \"don't care\", \"move on\", \"isn't important\", \"will not\"\n",
        "                         \"do not care\", \"could be worse\", \"no point\", \"does not matter\", \"is not important\"]\n",
        "    for phrase in apathetic_signals:\n",
        "        if phrase in text.lower():\n",
        "            feats[\"apathetic_signal\"] += 1\n",
        "\n",
        "    # --- Modals ---\n",
        "    modals = [\"must\", \"shall\", \"should\", \"need to\", \"ought to\", \"have to\"]\n",
        "    for modal in modals:\n",
        "        if modal in text.lower():\n",
        "            feats[\"modal_present\"] += 1\n",
        "\n",
        "    # --- Commands ---\n",
        "    imperatives = [\"stand up\", \"take back\", \"fight\", \"join\", \"resist\", \"reject\", \"act\"]\n",
        "    for imp in imperatives:\n",
        "        if imp in text.lower():\n",
        "            feats[\"imperative_present\"] += 1\n",
        "\n",
        "    # --- Militaristic vocabulary ---\n",
        "    militaristic = {\"troops\", \"attack\", \"defend\", \"enemy\", \"war\", \"mission\", \"strike\", \"military\",\n",
        "                    \"terrorist\", \"missile\", \"bomb\", \"gun\", \"police\", \"officer\", \"army\", \"soldier\"}\n",
        "    feats[\"military_vocab_count\"] = sum(1 for word in lowered if word in militaristic)\n",
        "\n",
        "    # --- Group Blame ---\n",
        "    blame_patterns = [\"they caused\", \"they ruined\", \"their fault\", \"they are the reason\", \"they must pay\", \"they made\", \"they did\"]\n",
        "    for pattern in blame_patterns:\n",
        "        if pattern in lowered_text:\n",
        "            feats[\"group_blaming_language\"] += 1\n",
        "\n",
        "    # -- Political Distrust --\n",
        "    politic_bad = [\"crooked\", \"corrupt\", \"rigged\", \"thieves\", \"woke\", \"radical\", \"racist\", \"bigotted\", \"manipulating\"]\n",
        "    for pattern in blame_patterns:\n",
        "        if pattern in lowered_text:\n",
        "            feats[\"political_distruct_language\"] += 1\n",
        "\n",
        "    # --- Extreme Language ---\n",
        "    extreme_patterns = [\"must kill\", \"kill them\", \"murder them\", \"they are animals\", \"radical left\", \"woke\",\n",
        "                        \"we will exterminate\", \"no right to live\", \"need to be wiped out\", \"they must suffer\"]\n",
        "    for pattern in extreme_patterns:\n",
        "        if pattern in lowered_text:\n",
        "            feats[\"extreme_pattern_match\"] += 1\n",
        "\n",
        "    terrorism_terms = {\"isis\", \"qaeda\", \"terror\", \"terrorists\"}\n",
        "\n",
        "    feats[\"terrorism_term\"] = int(any(word in terrorism_terms for word in lowered))\n",
        "    feats[\"has_hate_speech\"] = int(feats[\"hate_speech_vocab_count\"] > 0)\n",
        "    feats[\"has_violent_terms\"] = int(feats[\"violent_action_count\"] > 0)\n",
        "    feats[\"has_military_terms\"] = int(feats[\"military_vocab_count\"] > 0)\n",
        "    feats[\"has_ideological_violence\"] = feats.get(\"ideological_violence_flag\", 0)\n",
        "\n",
        "    # # --- Minimiziers ---\n",
        "    # minimizers = [\"maybe\", \"probably\", \"not a big deal\", \"could be worse\"]\n",
        "    # for word in minimizers:\n",
        "    #     if word in text.lower():\n",
        "    #         feats[\"minimizer_present\"] += 1\n",
        "\n",
        "    # # --- Fact Based ---\n",
        "    # fact_based = {\"data\", \"policy\", \"evidence\", \"report\", \"analysis\", \"studies\", \"economic\", \"legal\", \"proven\"}\n",
        "    # feats[\"fact_based_count\"] = sum(1 for word in lowered if word in fact_based)\n",
        "    # feats[\"uses_fact_based_terms\"] = int(feats[\"fact_based_count\"] > 0)\n",
        "\n",
        "    # # --- Hopeful Terms ---\n",
        "    # hopeful_terms = {\"hope\", \"dream\", \"together\", \"unite\", \"justice\", \"peace\", \"love\", \"freedom\"}\n",
        "    # feats[\"positive_persuasion\"] = sum(1 for word in lowered if word in hopeful_terms)\n",
        "\n",
        "    # racial_terms = {\"white\", \"black\", \"blacks\", \"muslims\", \"negro\", \"jews\", \"latinos\", \"viet\", \"arab\"}\n",
        "    # oppression_terms = {\"segregation\", \"racism\", \"racist\", \"slavery\"}\n",
        "\n",
        "    # feats[\"racial_identity_term\"] = int(any(word in racial_terms for word in lowered))\n",
        "    # feats[\"oppression_term\"] = int(any(word in oppression_terms for word in lowered))\n",
        "\n",
        "    return feats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "zehouefX7qL9"
      },
      "outputs": [],
      "source": [
        "def confidence_intervals(accuracy, n, significance_level):\n",
        "    critical_value=(1-significance_level)/2\n",
        "    z_alpha=-1*norm.ppf(critical_value)\n",
        "    se=math.sqrt((accuracy*(1-accuracy))/n)\n",
        "    return accuracy-(se*z_alpha), accuracy+(se*z_alpha)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def example_classifications(classifier):\n",
        "  sentence = \"not a big deal\"\n",
        "  predicted_label, probs = classifier.predict_one(sentence)\n",
        "  print(f\"Predicted tone: {predicted_label}\")\n",
        "  print(f\"Probabilities: {probs}\")\n",
        "\n",
        "  sentence = \"We the people.\"\n",
        "  predicted_label, probs = classifier.predict_one(sentence)\n",
        "  print(f\"Predicted tone: {predicted_label}\")\n",
        "  print(f\"Probabilities: {probs}\")\n",
        "\n",
        "  sentence = \"This is an important problem, and they are the ones to blame\"\n",
        "  predicted_label, probs = classifier.predict_one(sentence)\n",
        "  print(f\"Predicted tone: {predicted_label}\")\n",
        "  print(f\"Probabilities: {probs}\")\n",
        "\n",
        "  sentence = \"The radical left knows exactly what they're doing. They're ruthless and it's time that somebody did something about it.\"\n",
        "  predicted_label, probs = classifier.predict_one(sentence)\n",
        "\n",
        "  print(f\"Predicted tone: {predicted_label}\")\n",
        "  print(f\"Probabilities: {probs}\")\n",
        "\n",
        "def test_all_models(classifier):\n",
        "\n",
        "  rule_only_dev = classifier.example_test(dataset=\"dev\", use_rule_only=True)\n",
        "  print(f\"Rule-only dev accuracy: {rule_only_dev:.3f}\")\n",
        "\n",
        "  rule_only_test = classifier.example_test(dataset=\"test\", use_rule_only=True)\n",
        "  print(f\"Rule-only test accuracy: {rule_only_test:.3f}\")\n",
        "\n",
        "  baseline_dev = classifier.example_test(dataset=\"dev\", use_override=False)\n",
        "  print(f\"Model-only dev accuracy: {baseline_dev:.3f}\")\n",
        "\n",
        "  baseline_test = classifier.example_test(dataset=\"test\", use_override=False)\n",
        "  print(f\"Model-only test accuracy: {baseline_test:.3f}\")\n",
        "\n",
        "  dev_acc = classifier.example_test(dataset=\"dev\", use_override=True)\n",
        "  print(f\"Hybrid dev accuracy: {dev_acc:.3f}\")\n",
        "\n",
        "  test_acc = classifier.example_test(dataset=\"test\", use_override=True)\n",
        "  print(f\"Hybrid test accuracy: {test_acc:.3f}\")"
      ],
      "metadata": {
        "id": "WkeW9BmriXEN"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "id": "TwSlD-Cc7qL9"
      },
      "outputs": [],
      "source": [
        "def run(trainingFile, devFile, testFile, ordinal_values):\n",
        "\n",
        "    trainX, trainY, orig_trainY=load_ordinal_data(trainingFile, ordinal_values)\n",
        "    devX, devY, orig_devY=load_ordinal_data(devFile, ordinal_values)\n",
        "    testX, testY, orig_testY=load_ordinal_data(testFile, ordinal_values)\n",
        "\n",
        "    simple_classifier = OrdinalClassifier(ordinal_values, improved_bow_featurize, trainX, trainY, devX, devY, testX, testY, orig_trainY, orig_devY, orig_testY)\n",
        "    simple_classifier.train()\n",
        "    accuracy=simple_classifier.test()\n",
        "\n",
        "    lower, upper=confidence_intervals(accuracy, len(testY[0]), .95)\n",
        "    print(\"Test accuracy for best dev model:\\n%.3f, 95%% Confidence Intervals: [%.3f %.3f]\\n\" % (accuracy, lower, upper))\n",
        "\n",
        "    # example_classifications(simple_classifier)\n",
        "    print(\"Testing other variations of the model and reporting their accuracies:\\n\")\n",
        "    test_all_models(simple_classifier)\n",
        "\n",
        "    return simple_classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G7aOB6N07qL-",
        "outputId": "98e5f679-f2c4-4aa1-bec2-44d072813107"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy for best dev model:\n",
            "0.700, 95% Confidence Intervals: [0.610 0.790]\n",
            "\n",
            "Testing other variations of the model and reporting their accuracies:\n",
            "\n",
            "Rule-only dev accuracy: 0.410\n",
            "Rule-only test accuracy: 0.560\n",
            "Model-only dev accuracy: 0.640\n",
            "Model-only test accuracy: 0.700\n",
            "Hybrid dev accuracy: 0.560\n",
            "Hybrid test accuracy: 0.710\n"
          ]
        }
      ],
      "source": [
        "trainingFile = \"drive/MyDrive/train.txt\"\n",
        "devFile = \"drive/MyDrive/dev.txt\"\n",
        "testFile = \"drive/MyDrive/test.txt\"\n",
        "\n",
        "ordinal_values=[\"Apathetic\", \"Moderate\", \"Strongly Worded\", \"Extreme\"]\n",
        "\n",
        "classifier = run(trainingFile, devFile, testFile, ordinal_values)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Analysis**"
      ],
      "metadata": {
        "id": "6umXIrx4OFwr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What labels are often mistaken for each other -- Confusion Matrix:\n",
        "\n"
      ],
      "metadata": {
        "id": "KJl82BxhOaX3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_confusion_matrix(classifier):\n",
        "  y_true, y_pred = classifier.get_predictions()\n",
        "  cm = confusion_matrix(y_true, y_pred, labels=classifier.ordinal_values)\n",
        "  disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=classifier.ordinal_values)\n",
        "  disp.plot(cmap=\"Blues\")\n",
        "\n",
        "plot_confusion_matrix(classifier)"
      ],
      "metadata": {
        "id": "7fRlWnhuOgGI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "outputId": "dae51c1c-d4af-4773-97a2-d261eb9ca6f7"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "0 Apathetic Predicted, 0 Extreme Predicted:\n",
        "  - Reason for this is probably that not as much data for these categories to train our model on, leads to it over predicting the more prevalent labels (Moderate and Strongly Worded)\n",
        "  - Might need to find more features that help differentiate between strongly worded and extreme since most extremes were predicted to be Strongly Worded."
      ],
      "metadata": {
        "id": "wTrnCbSGc5eU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The confusion matrix revealed important patterns in the model’s performance. Notably, there were zero Apathetic and zero Extreme predictions across the entire test set. This suggests that the model is heavily biased toward predicting the more frequent categories (Moderate and Strongly Worded) which clearly reflects the distribution of the training data we provided. With fewer Apathetic and Extreme examples available during training, the model lacked sufficient examples to learn strong, distinguishing decision boundaries for these rarer classes. As a result, ambiguous or borderline examples were systematically pushed toward the more common categories. Additionally, many true Extreme examples were predicted as Strongly Worded. This indicates that while the model can recognize intensity, it struggles to separate violent rhetoric (Extreme) from strong but non-violent rhetoric (Strongly Worded)."
      ],
      "metadata": {
        "id": "7J8Aojee4Zou"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What features are learned to most define the classes?"
      ],
      "metadata": {
        "id": "YreJloiTOgmi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classifier.print_feature_importance(high_only = True, top_n = 10)"
      ],
      "metadata": {
        "id": "6B_nMw8MOku4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3765c5d4-149a-4854-b554-f258f6d894b2"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Feature importances for Moderate and above ---\n",
            "\n",
            "Highest-weighted features:\n",
            "pronoun_they_group             0.093\n",
            "military_vocab_count           0.056\n",
            "pronoun_we_group               0.050\n",
            "word_people                    0.049\n",
            "has_military_terms             0.037\n",
            "pronoun_you                    0.034\n",
            "modal_present                  0.033\n",
            "imperative_present             0.031\n",
            "word_world                     0.029\n",
            "word_war                       0.027\n",
            "\n",
            "--- Feature importances for Strongly Worded and above ---\n",
            "\n",
            "Highest-weighted features:\n",
            "word_white                     1.156\n",
            "word_negro                     0.971\n",
            "word_longer                    0.847\n",
            "word_time                      0.763\n",
            "word_years                     0.740\n",
            "word_segregation               0.725\n",
            "word_racist                    0.679\n",
            "word_racism                    0.675\n",
            "word_struggle                  0.653\n",
            "word_government                0.642\n",
            "\n",
            "--- Feature importances for Extreme and above ---\n",
            "\n",
            "Highest-weighted features:\n",
            "violent_action_count           0.298\n",
            "terrorism_term                 0.270\n",
            "imperative_present             0.224\n",
            "word_act                       0.215\n",
            "word_hang                      0.213\n",
            "word_throughout                0.211\n",
            "word_segregation               0.198\n",
            "word_immediately               0.197\n",
            "word_ever                      0.195\n",
            "word_terrorists                0.195\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The majority of the features that the model learned with a high frequency were expected; nevertheless, there are some features that were learned that came as a surprise to us. For Moderate text, the model learned that the presence of collective pronouns like \"they\" and \"we\" (pronoun_they_group, pronoun_we_group) was highly predictive. However, it was somewhat surprising that military vocabulary (military_vocab_count, has_military_terms) also had a positive weight toward Moderate. We originally anticipated military language would be more indicative of Strongly Worded or Extreme rhetoric, but the model suggests that references to military or national security are common even in moderate political discourse without necessarily conveying aggression.\n",
        "\n",
        "For Strongly Worded text, the model heavily emphasized racial and historical terms such as \"white\", \"negro\", \"segregation\", \"racism\", and \"struggle\". This was largely expected, given that many strongly worded political speeches historically reference racial injustice and societal divides with assertive language. However, the magnitude of importance placed on racial identifiers was larger than we had initially anticipated, suggesting that the model strongly associates Strongly Worded tone with discussions of racial events, even when explicit violent rhetoric is absent.\n",
        "\n",
        "For Extreme text, the model correctly prioritized features related to violence and terror, such as violent_action_count, terrorism_term, and words like \"act\", \"hang\", \"terrorists\", and \"immediately\". This matches our expectations as we predcited that references to violence, calls for immediate radical action, and the use of terrorism-related language would be highly indicative of Extreme tone. Interestingly, the model assigned notable weight to the word \"throughout\" as well, potentially capturing how extreme rhetoric sometimes uses sweeping generalizations across populations or geographic spaces."
      ],
      "metadata": {
        "id": "vxifG7IZjloJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What kind of systematic mistakes does your model make?"
      ],
      "metadata": {
        "id": "m1DoYtkPOlZh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def print_misclassified_examples(classifier):\n",
        "    preds = [lr.predict_proba(classifier.testX)[:, 1] for lr in classifier.log_regs]\n",
        "    preds = np.array(preds)\n",
        "    confusion_pairs = Counter()\n",
        "\n",
        "    for i in range(len(preds[0])):\n",
        "        ordinal_probs = np.zeros(len(classifier.ordinal_values))\n",
        "        ordinal_probs[0] = 1 - preds[0][i]\n",
        "        for j in range(1, len(classifier.ordinal_values) - 1):\n",
        "            ordinal_probs[j] = preds[j - 1][i] - preds[j][i]\n",
        "        ordinal_probs[-1] = preds[-1][i]\n",
        "\n",
        "        predicted_idx = np.argmax(ordinal_probs)\n",
        "        gold_idx = classifier.ordinal_values.index(classifier.orig_testY[i])\n",
        "\n",
        "        if predicted_idx != gold_idx:\n",
        "            gold_label = classifier.ordinal_values[gold_idx]\n",
        "            predicted_label = classifier.ordinal_values[predicted_idx]\n",
        "            confusion_pairs[(gold_label, predicted_label)] += 1\n",
        "\n",
        "    print(\"Most common confusion pairs (gold → predicted):\")\n",
        "    for (gold, pred), count in confusion_pairs.most_common():\n",
        "        print(f\"{gold:20s} → {pred:20s}: {count} times\")\n",
        "\n",
        "print_misclassified_examples(classifier)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vj7OGBI4Ot4k",
        "outputId": "47924d8e-fd8a-4a29-ff31-85bd9160e0ab"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most common confusion pairs (gold → predicted):\n",
            "Moderate             → Strongly Worded     : 14 times\n",
            "Extreme              → Strongly Worded     : 9 times\n",
            "Strongly Worded      → Moderate            : 5 times\n",
            "Apathetic            → Moderate            : 1 times\n",
            "Extreme              → Moderate            : 1 times\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We observed that the model tended to systematically confuse examples labeled as “Moderate” or “Extreme” with “Strongly Worded”, showing it struggles to distinguish between different intensities of emotional language. For instance, strongly emotional texts such as “And it is a war of unparalleled brutality…” and “Historically, revolutions are bloody, oh yes they are..” were both gold-labeled as “Extreme” but incorrectly predicted as “Strongly Worded”, while more moderate texts like “And going forward, it is essential that Pakistan continue to join us in the fight against al Qaeda..” and “Your struggles are our struggles” were also misclassified the same way. Even intense language, like \"We fight like Hell and if you don't fight like Hell, you're not going to have a country anymore,\" was marked as “Strongly Worded” instead of “Extreme.” Occasionally, it also confused apathetic statements, such as \"We must begin by acknowledging the hard truth: We will not eradicate violent conflict in our lifetimes,\" with “Moderate.” Overall, these mistakes suggest the model has a hard time detecting fine-grained differences in intensity and often leans toward predicting middle-ground labels like “Strongly Worded” when faced with both moderate and extreme input."
      ],
      "metadata": {
        "id": "n2SlY29CjkLP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Is one label extremely prevalent? How could this impact the model you developed? Is your dataset a good candidate for strategies like oversampling or changing class weights?"
      ],
      "metadata": {
        "id": "eedjGjuoOx4-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_true, y_pred = classifier.get_predictions()\n",
        "\n",
        "predicted_counts = Counter(y_pred)\n",
        "\n",
        "print(\"Number of times each label was predicted:\")\n",
        "for label in classifier.ordinal_values:\n",
        "    print(f\"{label:20s}: {predicted_counts[label]} times\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ayQkblzQdnt_",
        "outputId": "cb797bbb-e22d-413c-a48a-d567cd1a14da"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of times each label was predicted:\n",
            "Apathetic           : 0 times\n",
            "Moderate            : 44 times\n",
            "Strongly Worded     : 56 times\n",
            "Extreme             : 0 times\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clear that apathetic and extreme are either not as prevalent due to being on the extremes in terms of rhetoric or because our way of classifying what qualifies as either is not clear or might be too specific.\n",
        "- This could be a problem with Strongly Worded and Moderate being too vague as some excerpts which would normally be classified on either extreme don't fall under either extreme or apathetic due to bad class weighting.\n",
        "- Could also be a problem with how most of our examples fall under Strongly Worded or Moderate, not enough examples of Apathetic and Extreme in our training data."
      ],
      "metadata": {
        "id": "MJD3nopnd3Sr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The moderate and strongly worded labels are extremely prevalent in our dataset. This leads the model to heavily predict either Moderate or Strongly Worded. Some runs of the model even predict Apathetic or Extreme 0 times. We determined that our dataset did not have enough Apathetic or Extreme quotes for our model to accurately predict them. Another thing that could have contributed to this is that political speeches are often in a Moderate or Strongly Worded tone. Politicians most likely don’t want to come across like they don’t care about an issue, or that their stance is too extreme, so it would make sense for most political speeches to lie in the middle."
      ],
      "metadata": {
        "id": "MRpadbQ41EHy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What improvements can be made to this model?"
      ],
      "metadata": {
        "id": "ViM1mhfz0srh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our model ultimately ended up getting a 70% accuracy on the test set, altough this is quite high, considering our baseline was 38%, there is definitely still some room for improvement. Including more speeches from a wider range of political eras within the training set would provide a wider range of political perspective which would furhter assist the model with accurately predicting tone regardless of time-period. Furthermore, including speeches from different political affiliations, especially those on either extreme of the political spectrum, would allow for better collection and representation of both apathetic and extreme labels which would in turn help the model better determine those labels. To improve on the analysis itself we could add more features and implement Cross-Validation and analyzing VIFs for tuning hyper-parameters."
      ],
      "metadata": {
        "id": "UMpBuVxP0xE0"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZBPKBMbK0wgE"
      },
      "execution_count": 88,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}